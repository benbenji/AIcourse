{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"x1\":[1, 1, 1, 1, 1, 2, 2, 2, 2 ,2 ,3, 3, 3, 3, 3],\"x2\":['S','M','M','S','S','S','M','M','L','L','L','M','M','L','L'],\"Y\":[-1, -1, 1, 1, -1, -1, -1, 1, 1,1, 1, 1, 1, 1, -1]}\n",
    "data = pd.DataFrame(data_dict,index=range(1,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>S</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>M</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>M</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>S</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>S</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>M</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>M</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3</td>\n      <td>M</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>M</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3</td>\n      <td>L</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    x1 x2  Y\n1    1  S -1\n2    1  M -1\n3    1  M  1\n4    1  S  1\n5    1  S -1\n6    2  S -1\n7    2  M -1\n8    2  M  1\n9    2  L  1\n10   2  L  1\n11   3  L  1\n12   3  M  1\n13   3  M  1\n14   3  L  1\n15   3  L -1"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3    0.333333\n2    0.333333\n1    0.333333\nName: x1, dtype: float64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"x1\"].value_counts()/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_Y = data[data[\"Y\"] == 1]\n",
    "neg_Y = data[data[\"Y\"] == -1]\n",
    "prob_of_pos_Y = len(pos_Y) / len(data)\n",
    "prob_of_neg_Y = len(neg_Y) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.4"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_neg_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_pos_Y = pos_Y[\"x1\"].value_counts() / len(pos_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3    0.444444\n2    0.333333\n1    0.222222\nName: x1, dtype: float64"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_pos_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "M    0.444444\nL    0.444444\nS    0.111111\nName: x2, dtype: float64"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_pos_Y = pos_Y[\"x2\"].value_counts() / len(pos_Y)\n",
    "x2_pos_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1    0.500000\n2    0.333333\n3    0.166667\nName: x1, dtype: float64"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_neg_Y = neg_Y[\"x1\"].value_counts() / len(neg_Y)\n",
    "x1_neg_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "S    0.500000\nM    0.333333\nL    0.166667\nName: x2, dtype: float64"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_neg_Y = neg_Y[\"x2\"].value_counts() / len(neg_Y)\n",
    "x2_neg_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "-1\n"
    }
   ],
   "source": [
    "def predict(test):\n",
    "    prob_of_Pos = prob_of_pos_Y * x1_pos_Y[test[0]] *x2_pos_Y[test[1]]\n",
    "    prob_of_Neg = prob_of_neg_Y * x1_neg_Y[test[0]] *x2_neg_Y[test[1]]\n",
    "    return 1 if prob_of_Pos > prob_of_Neg else -1\n",
    "\n",
    "print(predict((2,'S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Help on class GaussianNB in module sklearn.naive_bayes:\n\nclass GaussianNB(BaseNB)\n |  GaussianNB(priors=None)\n |  \n |  Gaussian Naive Bayes (GaussianNB)\n |  \n |  Can perform online updates to model parameters via `partial_fit` method.\n |  For details on algorithm used to update feature means and variance online,\n |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n |  \n |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n |  \n |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n |  \n |  Parameters\n |  ----------\n |  priors : array-like, shape (n_classes,)\n |      Prior probabilities of the classes. If specified the priors are not\n |      adjusted according to the data.\n |  \n |  Attributes\n |  ----------\n |  class_prior_ : array, shape (n_classes,)\n |      probability of each class.\n |  \n |  class_count_ : array, shape (n_classes,)\n |      number of training samples observed in each class.\n |  \n |  theta_ : array, shape (n_classes, n_features)\n |      mean of each feature per class\n |  \n |  sigma_ : array, shape (n_classes, n_features)\n |      variance of each feature per class\n |  \n |  Examples\n |  --------\n |  >>> import numpy as np\n |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n |  >>> from sklearn.naive_bayes import GaussianNB\n |  >>> clf = GaussianNB()\n |  >>> clf.fit(X, Y)\n |  GaussianNB(priors=None)\n |  >>> print(clf.predict([[-0.8, -1]]))\n |  [1]\n |  >>> clf_pf = GaussianNB()\n |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n |  GaussianNB(priors=None)\n |  >>> print(clf_pf.predict([[-0.8, -1]]))\n |  [1]\n |  \n |  Method resolution order:\n |      GaussianNB\n |      BaseNB\n |      abc.NewBase\n |      sklearn.base.BaseEstimator\n |      sklearn.base.ClassifierMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, priors=None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, X, y, sample_weight=None)\n |      Fit Gaussian Naive Bayes according to X, y\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape (n_samples, n_features)\n |          Training vectors, where n_samples is the number of samples\n |          and n_features is the number of features.\n |      \n |      y : array-like, shape (n_samples,)\n |          Target values.\n |      \n |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n |          Weights applied to individual samples (1. for unweighted).\n |      \n |          .. versionadded:: 0.17\n |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n |      \n |      Returns\n |      -------\n |      self : object\n |          Returns self.\n |  \n |  partial_fit(self, X, y, classes=None, sample_weight=None)\n |      Incremental fit on a batch of samples.\n |      \n |      This method is expected to be called several times consecutively\n |      on different chunks of a dataset so as to implement out-of-core\n |      or online learning.\n |      \n |      This is especially useful when the whole dataset is too big to fit in\n |      memory at once.\n |      \n |      This method has some performance and numerical stability overhead,\n |      hence it is better to call partial_fit on chunks of data that are\n |      as large as possible (as long as fitting in the memory budget) to\n |      hide the overhead.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape (n_samples, n_features)\n |          Training vectors, where n_samples is the number of samples and\n |          n_features is the number of features.\n |      \n |      y : array-like, shape (n_samples,)\n |          Target values.\n |      \n |      classes : array-like, shape (n_classes,), optional (default=None)\n |          List of all the classes that can possibly appear in the y vector.\n |      \n |          Must be provided at the first call to partial_fit, can be omitted\n |          in subsequent calls.\n |      \n |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n |          Weights applied to individual samples (1. for unweighted).\n |      \n |          .. versionadded:: 0.17\n |      \n |      Returns\n |      -------\n |      self : object\n |          Returns self.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseNB:\n |  \n |  predict(self, X)\n |      Perform classification on an array of test vectors X.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = [n_samples, n_features]\n |      \n |      Returns\n |      -------\n |      C : array, shape = [n_samples]\n |          Predicted target values for X\n |  \n |  predict_log_proba(self, X)\n |      Return log-probability estimates for the test vector X.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = [n_samples, n_features]\n |      \n |      Returns\n |      -------\n |      C : array-like, shape = [n_samples, n_classes]\n |          Returns the log-probability of the samples for each class in\n |          the model. The columns correspond to the classes in sorted\n |          order, as they appear in the attribute `classes_`.\n |  \n |  predict_proba(self, X)\n |      Return probability estimates for the test vector X.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = [n_samples, n_features]\n |      \n |      Returns\n |      -------\n |      C : array-like, shape = [n_samples, n_classes]\n |          Returns the probability of the samples for each class in\n |          the model. The columns correspond to the classes in sorted\n |          order, as they appear in the attribute `classes_`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : boolean, optional\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : mapping of string to any\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as pipelines). The latter have parameters of the form\n |      ``<component>__<parameter>`` so that it's possible to update each\n |      component of a nested object.\n |      \n |      Returns\n |      -------\n |      self\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.BaseEstimator:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Returns the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n |          True labels for X.\n |      \n |      sample_weight : array-like, shape = [n_samples], optional\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of self.predict(X) wrt. y.\n\n"
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['1' 'S']\n ['1' 'M']\n ['1' 'M']\n ['1' 'S']\n ['1' 'S']\n ['2' 'S']\n ['2' 'M']\n ['2' 'M']\n ['2' 'L']\n ['2' 'L']\n ['3' 'L']\n ['3' 'M']\n ['3' 'M']\n ['3' 'L']\n ['3' 'L']]\n"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-6bb0fe333007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# clf.predict((2,'S'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 185\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# boost the variance by epsilon, a small fraction of the standard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# deviation of the largest dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-9\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3157\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Note that if dtype is not of inexact type then arraymean will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# not be either.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0marrmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         arrmean = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([[1, 1, 1, 1, 1, 2, 2, 2, 2 ,2 ,3, 3, 3, 3, 3],['S','M','M','S','S','S','M','M','L','L','L','M','M','L','L']])\n",
    "x = x.T\n",
    "print(x)\n",
    "y=np.array([-1, -1, 1, 1, -1, -1, -1, 1, 1,1, 1, 1, 1, 1, -1])\n",
    "clf = GaussianNB()\n",
    "clf.fit(x,y)\n",
    "# clf.predict((2,'S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}